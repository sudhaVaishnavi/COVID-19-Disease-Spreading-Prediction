# Import Libraries
from google.colab import files
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, GRU, LSTM, Bidirectional, Conv1D, MaxPooling1D
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler

# Step 1: Upload the Dataset
uploaded = files.upload()
print("Uploaded file:", uploaded.keys())

# Load the dataset (assumes the uploaded file is named 'covid_19_clean_complete.csv')
data = pd.read_csv(list(uploaded.keys())[0])

# Step 2: Data Preprocessing
# Convert Date column to datetime format
data['Date'] = pd.to_datetime(data['Date'])

# Sort by Date and Country/Region
data = data.sort_values(by=['Country/Region', 'Date'])

# Feature Engineering - Calculate daily new cases
data['DailyConfirmed'] = data.groupby(['Country/Region'])['Confirmed'].diff().fillna(0)

# Handling Sharp Spikes: Apply log transformation to DailyConfirmed
# To avoid log(0) and log of negative values, replace zeros with a small value
data['DailyConfirmed'] = data['DailyConfirmed'].apply(lambda x: x if x > 0 else 1)

# Apply log1p transformation
data['DailyConfirmed'] = np.log1p(data['DailyConfirmed'])

# Add additional lag features (e.g., 3-day and 7-day lag)
data['Lag3'] = data['DailyConfirmed'].shift(3).fillna(0)
data['Lag7'] = data['DailyConfirmed'].shift(7).fillna(0)

# Add rolling averages (e.g., 3-day and 7-day moving average)
data['RollingAvg3'] = data['DailyConfirmed'].rolling(3).mean().fillna(0)
data['RollingAvg7'] = data['DailyConfirmed'].rolling(7).mean().fillna(0)

# Select Data for Modeling (e.g., focus on a specific country, or aggregate data)
# For simplicity, let's aggregate data globally
global_data = data.groupby('Date').sum().reset_index()

# Step 3: Create Features and Target Variables
# Use previous 7 days' cases and additional features (lags and rolling averages) to predict the next day's cases

window_size = 7
X, y = [], []
for i in range(window_size, len(global_data)):
    X.append(global_data[['DailyConfirmed', 'Lag3', 'Lag7', 'RollingAvg3', 'RollingAvg7']].iloc[i-window_size:i].values)
    y.append(global_data['DailyConfirmed'].iloc[i])

# Convert lists to numpy arrays for model input
X, y = np.array(X), np.array(y)

# Reshape X to fit LSTM/GRU input shape (samples, time steps, features)
X = X.reshape((X.shape[0], X.shape[1], X.shape[2]))

# Split Data into Train and Test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Scale Data for LSTM/GRU
scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()

X_train = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[1] * X_train.shape[2]))
X_test = scaler_X.transform(X_test.reshape(-1, X_test.shape[1] * X_test.shape[2]))

X_train = X_train.reshape(X_train.shape[0], window_size, X_train.shape[1] // window_size)
X_test = X_test.reshape(X_test.shape[0], window_size, X_test.shape[1] // window_size)

y_train = y_train.reshape(-1, 1)
y_test = y_test.reshape(-1, 1)

y_train = scaler_y.fit_transform(y_train)
y_test = scaler_y.transform(y_test)

# Step 4: Define the GRU Model with Bidirectional Layers and Convolutional Layers
model = Sequential([
    Conv1D(64, 3, activation='relu', input_shape=(window_size, X_train.shape[2])),
    MaxPooling1D(),
    Dropout(0.2),
    Bidirectional(GRU(128, activation='relu', return_sequences=True)),
    Dropout(0.2),
    Bidirectional(GRU(64, activation='relu', return_sequences=False)),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)  # Output layer with 1 neuron (predicting a single value)
])

# Early stopping to avoid overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Learning rate scheduler
def scheduler(epoch, lr):
    if epoch > 10:
        return lr * 0.9  # Reduce learning rate after 10 epochs
    return lr

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

# Step 5: Train the Model
history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test),
                    callbacks=[early_stopping, LearningRateScheduler(scheduler)])

# Step 6: Evaluate the Model
loss, mae = model.evaluate(X_test, y_test)
print(f"Test MAE: {mae}")

# Predictions
y_pred = model.predict(X_test)

# Rescale predictions and actual values to original scale (Inverse scaling)
y_pred_rescaled = scaler_y.inverse_transform(y_pred)
y_test_rescaled = scaler_y.inverse_transform(y_test)

# Calculate R² (coefficient of determination)
r2 = r2_score(y_test_rescaled, y_pred_rescaled)
print(f"R² Score: {r2}")

# Step 7: Plot Actual vs Predicted Cases
plt.figure(figsize=(10,6))
plt.plot(y_test_rescaled, label='Actual Daily Cases')
plt.plot(y_pred_rescaled, label='Predicted Daily Cases')
plt.xlabel('Days')
plt.ylabel('Daily Confirmed Cases')
plt.legend()
plt.show()
